{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation Notebook\n",
    "This notebook evaluates a fine-tuned Audio Spectrogram Transformer (AST) model on an unseen audio dataset. It loads a pre-trained model and uses its feature extractor to process raw audio directly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from datasets import load_dataset, Audio, DatasetDict\n",
    "from transformers import ASTFeatureExtractor, ASTForAudioClassification, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Evaluation Configuration ---\n",
    "DATASET_NAME = \"YOUR_EVALUATION_DATASET_NAME_OR_PATH\"  # E.g., 'username/my_test_audio_dataset' or a local path\n",
    "\n",
    "# --- Model & Cache Paths ---\n",
    "MODEL_HUB_ID = \"username/my_test_audio_dataset\"  # Path to the directory where the fine-tuned model was saved\n",
    "CACHE_DIR = './cache'\n",
    "\n",
    "# --- Audio Processing Parameters (should match training) ---\n",
    "TARGET_SAMPLE_RATE = 16000  # Hz (16kHz)\n",
    "CHUNK_LENGTH_MS = 1000      # milliseconds (1 second)\n",
    "CHUNK_LENGTH_SAMPLES = int(TARGET_SAMPLE_RATE * CHUNK_LENGTH_MS / 1000)\n",
    "\n",
    "# --- Evaluation Hyperparameters ---\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_dataset(dataset_name: str) -> DatasetDict:\n",
    "    \"\"\"Load and prepare the evaluation dataset.\"\"\"\n",
    "    try:\n",
    "        ds = load_dataset(dataset_name, split='train', cache_dir=CACHE_DIR)\n",
    "        # Ensure audio is at the target sample rate and mono\n",
    "        ds = ds.cast_column(\"audio\", Audio(sampling_rate=TARGET_SAMPLE_RATE, mono=True))\n",
    "        print(f\"Dataset {dataset_name} loaded successfully with {ds.num_rows} examples.\")\n",
    "            \n",
    "        return ds\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load or prepare dataset {dataset_name}: {e}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "recall_metric = evaluate.load(\"recall\")\n",
    "precision_metric = evaluate.load(\"precision\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "AVERAGE_MODE = \"binary\"  # For binary classification\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    metrics = {}\n",
    "    metrics.update(accuracy_metric.compute(predictions=predictions, references=labels))\n",
    "    metrics.update(precision_metric.compute(predictions=predictions, references=labels, average=AVERAGE_MODE))\n",
    "    metrics.update(recall_metric.compute(predictions=predictions, references=labels, average=AVERAGE_MODE))\n",
    "    metrics.update(f1_metric.compute(predictions=predictions, references=labels, average=AVERAGE_MODE))\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Fine-tuned Model, Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Load model and feature extractor from Hugging Face Hub ID\n",
    "    print(f\"Attempting to load model and feature extractor from Hugging Face Hub ID: {MODEL_HUB_ID}\")\n",
    "    model = ASTForAudioClassification.from_pretrained(MODEL_HUB_ID, cache_dir=CACHE_DIR)\n",
    "    # It's good practice to explicitly set sampling_rate for feature_extractor if known\n",
    "    feature_extractor = ASTFeatureExtractor.from_pretrained(MODEL_HUB_ID, sampling_rate=TARGET_SAMPLE_RATE)\n",
    "    print(f\"Successfully loaded model and feature extractor from {MODEL_HUB_ID}\")\n",
    "    model.to(DEVICE)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model/feature extractor from {MODEL_HUB_ID}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Function for Evaluation\n",
    "This function processes raw audio using the loaded `ASTFeatureExtractor`. It truncates or pads audio to `CHUNK_LENGTH_SAMPLES` (1 second) before creating the spectrogram features, ensuring consistency with the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_for_evaluation(examples):\n",
    "    # 'examples['input_values']' is expected to be a list of audio dicts {'array': ..., 'sampling_rate': ...}\n",
    "    audio_arrays = [x[\"array\"] for x in examples['input_values']]\n",
    "    \n",
    "    inputs = feature_extractor(\n",
    "        audio_arrays,\n",
    "        sampling_rate=TARGET_SAMPLE_RATE, # Or directly use feature_extractor.sampling_rate\n",
    "        max_length=CHUNK_LENGTH_SAMPLES,  # Ensures audio is processed as 1-second segments\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    # The trainer expects the features to be in a column named 'input_values'\n",
    "    examples[\"input_values\"] = inputs.input_values\n",
    "    return examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_and_prepare_dataset(DATASET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the preprocessing\n",
    "dataset = dataset.rename_column('audio', 'input_values')\n",
    "processed_dataset = dataset.with_transform(preprocess_for_evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer Initialization and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./eval_results\",\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    do_train=False,\n",
    "    do_eval=True,\n",
    "    report_to=\"none\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    eval_dataset=processed_dataset,\n",
    "    processing_class=feature_extractor,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting evaluation...\")\n",
    "results = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Evaluation Results ---\")\n",
    "for key, value in results.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

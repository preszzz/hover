{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "This notebook evaluates a fine-tuned Audio Spectrogram Transformer (AST) model on an unseen audio dataset. It loads a pre-trained model and uses its feature extractor to process raw audio directly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import evaluate\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from datasets import load_dataset, Audio, Dataset\n",
    "from transformers import ASTFeatureExtractor, ASTForAudioClassification, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Evaluation Configuration ---\n",
    "DATASET_NAME = \"YOUR_EVALUATION_DATASET_NAME_OR_PATH\"  # E.g., 'username/my_test_audio_dataset' or a local path\n",
    "DATASET_SPLIT = 'train'\n",
    "CONFIG_NAME = 'ours'\n",
    "\n",
    "# --- Model & Cache Paths ---\n",
    "MODEL_HUB_ID = \"username/my_test_audio_dataset\"  # Path to the directory where the fine-tuned model was saved\n",
    "CACHE_DIR = './cache'\n",
    "\n",
    "# --- Audio Processing Parameters (should match training) ---\n",
    "TARGET_SAMPLE_RATE = 16000  # Hz (16kHz)\n",
    "CHUNK_LENGTH_MS = 1000      # milliseconds (1 second)\n",
    "CHUNK_LENGTH_SAMPLES = int(TARGET_SAMPLE_RATE * CHUNK_LENGTH_MS / 1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_dataset(dataset_name: str) -> Dataset:\n",
    "    \"\"\"Load and prepare the evaluation dataset.\"\"\"\n",
    "    try:\n",
    "        ds = load_dataset(dataset_name, CONFIG_NAME, split=DATASET_SPLIT, cache_dir=CACHE_DIR)\n",
    "        # Ensure audio is at the target sample rate and mono\n",
    "        ds = ds.cast_column(\"audio\", Audio(sampling_rate=TARGET_SAMPLE_RATE, mono=True))\n",
    "        print(f\"Dataset {dataset_name} loaded successfully with config {CONFIG_NAME}: {ds.num_rows} examples.\")\n",
    "            \n",
    "        return ds\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load or prepare dataset {dataset_name}: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Fine-tuned Model, Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Load model and feature extractor from Hugging Face Hub ID\n",
    "    print(f\"Attempting to load model and feature extractor from Hugging Face Hub ID: {MODEL_HUB_ID}\")\n",
    "    model = ASTForAudioClassification.from_pretrained(MODEL_HUB_ID, cache_dir=CACHE_DIR)\n",
    "    feature_extractor = ASTFeatureExtractor.from_pretrained(MODEL_HUB_ID, sampling_rate=TARGET_SAMPLE_RATE)\n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model/feature extractor from {MODEL_HUB_ID}: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Function for Evaluation\n",
    "This function processes raw audio using the loaded `ASTFeatureExtractor`. It truncates or pads audio to `CHUNK_LENGTH_SAMPLES` (1 second) before creating the spectrogram features, ensuring consistency with the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_audio_array(audio_array: np.ndarray, chunk_length_samples: int) -> list[np.ndarray]:\n",
    "    \"\"\"Splits a long audio array into non-overlapping chunks of specified length.\"\"\"\n",
    "    num_samples = len(audio_array)\n",
    "    num_chunks = math.ceil(num_samples / chunk_length_samples)\n",
    "    chunks = []\n",
    "    for i in range(num_chunks):\n",
    "        start = i * chunk_length_samples\n",
    "        end = start + chunk_length_samples\n",
    "        chunk = audio_array[start:end]\n",
    "        \n",
    "        # Pad the last chunk if it's shorter than chunk_length_samples\n",
    "        if len(chunk) < chunk_length_samples:\n",
    "            padding_needed = chunk_length_samples - len(chunk)\n",
    "            # Simple zero padding at the end\n",
    "            chunk = np.pad(chunk, (0, padding_needed), 'constant')\n",
    "        \n",
    "        chunks.append(chunk)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_and_prepare_dataset(DATASET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_true_file_labels = []\n",
    "all_aggregated_file_predictions = [] # Store 0 for not_drone, 1 for drone\n",
    "all_file_drone_scores = []\n",
    "\n",
    "# For binary: 0 -> not_drone, 1 -> drone.\n",
    "DRONE_CLASS_INDEX = 1\n",
    "\n",
    "print(\"Starting custom evaluation loop...\")\n",
    "for example in dataset:\n",
    "    \n",
    "    audio_data = example['audio']\n",
    "    raw_audio_array = audio_data['array']\n",
    "    true_file_label = example['label'] \n",
    "    \n",
    "    print(f\"Begin processing: {audio_data['path']}\")\n",
    "\n",
    "    all_true_file_labels.append(true_file_label)\n",
    "    \n",
    "    audio_chunks = chunk_audio_array(raw_audio_array, CHUNK_LENGTH_SAMPLES)\n",
    "    \n",
    "    file_predicted_as_drone = False\n",
    "    max_drone_prob_for_file_score = 0.0 \n",
    "\n",
    "    for chunk_array in audio_chunks:\n",
    "        # Preprocess the chunk\n",
    "        inputs = feature_extractor(\n",
    "            [chunk_array], # Feature extractor expects a list of arrays\n",
    "            sampling_rate=TARGET_SAMPLE_RATE,\n",
    "            max_length=CHUNK_LENGTH_SAMPLES,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        input_values = inputs.input_values.to(DEVICE)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = model(input_values).logits\n",
    "        \n",
    "        predicted_class_idx = torch.argmax(logits).item()\n",
    "        \n",
    "        if predicted_class_idx == DRONE_CLASS_INDEX:\n",
    "            file_predicted_as_drone = True\n",
    "        \n",
    "        # For continuous score\n",
    "        probabilities_chunk = torch.softmax(logits, dim=-1)[0]\n",
    "        drone_prob_for_chunk = probabilities_chunk[DRONE_CLASS_INDEX].item()\n",
    "        if drone_prob_for_chunk > max_drone_prob_for_file_score:\n",
    "            max_drone_prob_for_file_score = drone_prob_for_chunk\n",
    "            \n",
    "    all_aggregated_file_predictions.append(1 if file_predicted_as_drone else 0)\n",
    "    all_file_drone_scores.append(max_drone_prob_for_file_score)\n",
    "    print(f\"File prediction: {'Drone' if file_predicted_as_drone else 'Not Drone'}\")\n",
    "\n",
    "\n",
    "print(\"Evaluation loop finished.\")\n",
    "print(f\"Total files processed: {len(all_true_file_labels)}\")\n",
    "print(f\"Number of true labels: {sum(all_true_file_labels)}\")\n",
    "print(f\"Number of predicted files: {sum(all_aggregated_file_predictions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metrics\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "recall_metric = evaluate.load(\"recall\")\n",
    "precision_metric = evaluate.load(\"precision\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "AVERAGE_MODE = \"binary\"\n",
    "\n",
    "def compute_file_level_metrics(true_labels, predicted_labels):\n",
    "    metrics = {}\n",
    "    metrics.update(accuracy_metric.compute(predictions=predicted_labels, references=true_labels))\n",
    "    # For binary precision/recall/f1, ensure pos_label is correctly set if default (1) isn't what you want\n",
    "    # Or ensure your DRONE_CLASS_INDEX aligns with the positive label notion.\n",
    "    metrics.update(precision_metric.compute(predictions=predicted_labels, references=true_labels, average=AVERAGE_MODE, pos_label=DRONE_CLASS_INDEX))\n",
    "    metrics.update(recall_metric.compute(predictions=predicted_labels, references=true_labels, average=AVERAGE_MODE, pos_label=DRONE_CLASS_INDEX))\n",
    "    metrics.update(f1_metric.compute(predictions=predicted_labels, references=true_labels, average=AVERAGE_MODE, pos_label=DRONE_CLASS_INDEX))\n",
    "    return metrics\n",
    "\n",
    "# Convert lists to numpy arrays for the metrics functions\n",
    "true_labels_np = np.array(all_true_file_labels)\n",
    "aggregated_predictions_np = np.array(all_aggregated_file_predictions)\n",
    "file_scores_np = np.array(all_file_drone_scores)\n",
    "\n",
    "print(\"\\n--- File-Level Evaluation Results ---\")\n",
    "file_metrics = compute_file_level_metrics(true_labels_np, aggregated_predictions_np)\n",
    "for key, value in file_metrics.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# --- AUC Calculation (using CONTINUOUS scores) ---\n",
    "auc_score = roc_auc_score(true_labels_np, file_scores_np)\n",
    "print(f\"AUC: {auc_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer Initialization and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./eval_results\",\n",
    "#     per_device_eval_batch_size=BATCH_SIZE,\n",
    "#     do_train=False,\n",
    "#     do_eval=True,\n",
    "#     report_to=\"none\"\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

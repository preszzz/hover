{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install evaluate datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "FdR_wkQ-x5cI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import evaluate\n",
        "import numpy as np\n",
        "from datasets import load_dataset, Dataset, Audio\n",
        "from transformers import ASTFeatureExtractor, ASTForAudioClassification, ASTConfig, Trainer, TrainingArguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "mtelf10hw6mG"
      },
      "outputs": [],
      "source": [
        "DATASET_NAME = ''\n",
        "CACHE_DIR = './cache'\n",
        "ROOT_DIR = './'\n",
        "\n",
        "# --- Audio Processing Parameters ---\n",
        "TARGET_SAMPLE_RATE = 16000  # Hz (16kHz)\n",
        "CHUNK_LENGTH_MS = 1000      # milliseconds (1 second)\n",
        "CHUNK_LENGTH_SAMPLES = int(TARGET_SAMPLE_RATE * CHUNK_LENGTH_MS / 1000)\n",
        "\n",
        "# --- Training Hyperparameters ---\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 5\n",
        "LR = 3e-5\n",
        "\n",
        "# --- Model Configuration ---\n",
        "MODEL_CHECKPOINT = \"MIT/ast-finetuned-audioset-10-10-0.4593\"\n",
        "MODEL_SAVE_DIR = os.path.join(ROOT_DIR, 'output_models')\n",
        "CHECKPOINT_FILENAME = \"ast_best_model\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "XETJP0qBxYa7"
      },
      "outputs": [],
      "source": [
        "def load_dataset_splits(dataset_name: str) -> Dataset:\n",
        "    \"\"\"Load a dataset from the Hugging Face Hub.\n",
        "\n",
        "    Args:\n",
        "        dataset_name: Name of the dataset to load\n",
        "\n",
        "    Returns:\n",
        "        Hugging Face Dataset object\n",
        "    \"\"\"\n",
        "    try:\n",
        "        dataset = load_dataset(dataset_name, cache_dir=CACHE_DIR)\n",
        "        dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=TARGET_SAMPLE_RATE, mono=True))\n",
        "\n",
        "        print(f\"Dataset loaded successfully with splits: {list(dataset.keys())}\")\n",
        "        print(f\"Train split size: {dataset['train'].num_rows}\")\n",
        "        if 'valid' in dataset:\n",
        "            print(f\"Validation split size: {dataset['valid'].num_rows}\")\n",
        "        if 'test' in dataset:\n",
        "            print(f\"Test split size: {dataset['test'].num_rows}\")\n",
        "\n",
        "        return dataset\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to load dataset {dataset_name}: {e}\")\n",
        "        raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "K7-yjfZdx8t1"
      },
      "outputs": [],
      "source": [
        "def build_transformer_model(num_classes: int, model_checkpoint: str):\n",
        "    \"\"\"\n",
        "    Loads a pre-trained Audio Spectrogram Transformer (AST) model\n",
        "    from Hugging Face for PyTorch.\n",
        "\n",
        "    Args:\n",
        "        num_classes (int): The number of output classes for the classification layer.\n",
        "        model_checkpoint (str): The Hugging Face AST model identifier.\n",
        "\n",
        "    Returns:\n",
        "        torch.nn.Module: The PyTorch AST model with a potentially resized classification head.\n",
        "    \"\"\"\n",
        "    # Load the pre-trained AST model\n",
        "    # Set ignore_mismatched_sizes=True to allow replacing the classification head\n",
        "    label = {'not_drone': 0, 'drone': 1}\n",
        "\n",
        "    ast_config = ASTConfig.from_pretrained(model_checkpoint)\n",
        "\n",
        "    ast_config.num_labels = num_classes\n",
        "    ast_config.label2id = label\n",
        "    ast_config.id2label = {v: k for k, v in label.items()}\n",
        "\n",
        "    model = ASTForAudioClassification.from_pretrained(\n",
        "        model_checkpoint,\n",
        "        cache_dir=CACHE_DIR,\n",
        "        config=ast_config,\n",
        "        ignore_mismatched_sizes=True,\n",
        "    )\n",
        "    return model\n",
        "\n",
        "def get_feature_extractor(model_checkpoint: str):\n",
        "    \"\"\"\n",
        "    Loads the AST feature extractor.\n",
        "\n",
        "    Args:\n",
        "        model_checkpoint (str): The Hugging Face AST model identifier.\n",
        "\n",
        "    Returns:\n",
        "        ASTFeatureExtractor: The AST feature extractor instance.\n",
        "    \"\"\"\n",
        "    # Load the feature extractor\n",
        "    try:\n",
        "        feature_extractor = ASTFeatureExtractor.from_pretrained(\n",
        "            model_checkpoint,\n",
        "            sampling_rate=TARGET_SAMPLE_RATE\n",
        "        )\n",
        "        return feature_extractor\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading feature extractor: {e}\")\n",
        "        raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    feature_extractor = get_feature_extractor(MODEL_CHECKPOINT)\n",
        "    print(f\"Successfully loaded feature extractor: {feature_extractor}\")\n",
        "    print(f\"Target sampling rate from feature extractor: {feature_extractor.sampling_rate} Hz\")\n",
        "    print(f\"Feature extractor expects max_length: {feature_extractor.max_length} samples\")\n",
        "except Exception as e:\n",
        "    print(f\"Failed to load feature extractor: {e}\", exc_info=True)\n",
        "    feature_extractor = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "f94VTQqCyMOd"
      },
      "outputs": [],
      "source": [
        "def preprocess_features(example):\n",
        "    \"\"\"Applies the AST feature extractor to a batch of audio data.\n",
        "\n",
        "    This function is designed to be used with `dataset.with_transform()`.\n",
        "\n",
        "    Args:\n",
        "        batch: A dictionary representing a batch of examples from the HF dataset.\n",
        "        Expected to contain an 'audio' key with audio data.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing the processed 'input_values'.\n",
        "    \"\"\"\n",
        "    # Ensure audio data is in the expected format (list of numpy arrays)\n",
        "    audio_arrays = [x[\"array\"] for x in example['input_values']]\n",
        "\n",
        "    # Apply the feature extractor\n",
        "    inputs = feature_extractor(\n",
        "        audio_arrays,\n",
        "        sampling_rate=feature_extractor.sampling_rate,\n",
        "        max_length=CHUNK_LENGTH_SAMPLES,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    if \"input_values\" in inputs:\n",
        "        example[\"input_values\"] = inputs[\"input_values\"]\n",
        "    else:\n",
        "        print(f\"Feature extractor output did not contain expected keys ('input_values'). Found: {inputs.keys()}\")\n",
        "        # Handle error appropriately, maybe return None or raise exception\n",
        "        raise KeyError(\"Could not find processed features in feature extractor output.\")\n",
        "\n",
        "    return example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "bAUItyiVoYoF"
      },
      "outputs": [],
      "source": [
        "accuracy = evaluate.load(\"accuracy\")\n",
        "recall = evaluate.load(\"recall\")\n",
        "precision = evaluate.load(\"precision\")\n",
        "f1 = evaluate.load(\"f1\")\n",
        "\n",
        "AVERAGE = \"binary\"\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits = eval_pred.predictions\n",
        "    predictions = np.argmax(logits, axis=1)\n",
        "    metrics = accuracy.compute(predictions=predictions, references=eval_pred.label_ids)\n",
        "    metrics.update(precision.compute(predictions=predictions, references=eval_pred.label_ids, average=AVERAGE))\n",
        "    metrics.update(recall.compute(predictions=predictions, references=eval_pred.label_ids, average=AVERAGE))\n",
        "    metrics.update(f1.compute(predictions=predictions, references=eval_pred.label_ids, average=AVERAGE))\n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA available. Using GPU.\n"
          ]
        }
      ],
      "source": [
        "def get_device():\n",
        "    if torch.cuda.is_available():\n",
        "        print(\"CUDA available. Using GPU.\")\n",
        "        return torch.device(\"cuda\")\n",
        "    else:\n",
        "        print(\"CUDA/MPS not available. Using CPU.\")\n",
        "        return torch.device(\"cpu\")\n",
        "\n",
        "DEVICE = get_device()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ds = load_dataset_splits(dataset_name=DATASET_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dataset loaded successfully with splits: ['train', 'valid', 'test']\n",
        "Train split size: 585180\n",
        "Validation split size: 32510\n",
        "Test split size: 32511"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "flXFjQJuMJZ0"
      },
      "outputs": [],
      "source": [
        "ds = ds.rename_column('audio', 'input_values')\n",
        "processed_datasets = ds.with_transform(preprocess_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = build_transformer_model(num_classes=2, model_checkpoint=MODEL_CHECKPOINT)\n",
        "model.to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "RpFrbXYNXR9u"
      },
      "outputs": [],
      "source": [
        "args = TrainingArguments(\n",
        "    output_dir=\"test-train-model\",\n",
        "    eval_strategy = \"epoch\",\n",
        "    save_strategy = \"epoch\",\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    num_train_epochs=EPOCHS,\n",
        "    learning_rate=LR,\n",
        "    logging_steps=10,\n",
        "    gradient_accumulation_steps=4,\n",
        "    warmup_ratio=0.1,\n",
        "    metric_for_best_model=\"accuracy\",\n",
        "    load_best_model_at_end=True,\n",
        "    fp16=True,\n",
        "    disable_tqdm=False,\n",
        "    # push_to_hub=True,\n",
        "    # tpu_num_cores=48\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "xTRB62vMolp3"
      },
      "outputs": [],
      "source": [
        "# Setup the trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=processed_datasets[\"train\"],\n",
        "    eval_dataset=processed_datasets[\"valid\"],\n",
        "    processing_class=feature_extractor,\n",
        "    compute_metrics=compute_metrics,  # Use the metrics function from above\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/javascript": "\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mzh-preston\u001b[0m (\u001b[33mzh-preston-queen-s-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250510_053919-jw9tmngu</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/zh-preston-queen-s-university/huggingface/runs/jw9tmngu' target=\"_blank\">test-train-model</a></strong> to <a href='https://wandb.ai/zh-preston-queen-s-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/zh-preston-queen-s-university/huggingface' target=\"_blank\">https://wandb.ai/zh-preston-queen-s-university/huggingface</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/zh-preston-queen-s-university/huggingface/runs/jw9tmngu' target=\"_blank\">https://wandb.ai/zh-preston-queen-s-university/huggingface/runs/jw9tmngu</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='22855' max='22855' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [22855/22855 7:36:55, Epoch 4/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.001000</td>\n",
              "      <td>0.003579</td>\n",
              "      <td>0.998831</td>\n",
              "      <td>0.999057</td>\n",
              "      <td>0.999450</td>\n",
              "      <td>0.999253</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.001100</td>\n",
              "      <td>0.003091</td>\n",
              "      <td>0.999108</td>\n",
              "      <td>0.999725</td>\n",
              "      <td>0.999135</td>\n",
              "      <td>0.999430</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.002000</td>\n",
              "      <td>0.001945</td>\n",
              "      <td>0.999539</td>\n",
              "      <td>0.999646</td>\n",
              "      <td>0.999764</td>\n",
              "      <td>0.999705</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001803</td>\n",
              "      <td>0.999723</td>\n",
              "      <td>0.999803</td>\n",
              "      <td>0.999843</td>\n",
              "      <td>0.999823</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=22855, training_loss=0.004115778539612579, metrics={'train_runtime': 27434.7446, 'train_samples_per_second': 106.649, 'train_steps_per_second': 0.833, 'total_flos': 1.9828487041060543e+20, 'train_loss': 0.004115778539612579, 'epoch': 4.998961010553946})"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "OxnXKzZjrMBL"
      },
      "outputs": [],
      "source": [
        "trainer.save_model(MODEL_SAVE_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1016' max='1016' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1016/1016 02:22]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'eval_loss': 0.0018028703052550554,\n",
              " 'eval_accuracy': 0.999723162103968,\n",
              " 'eval_precision': 0.9998034745696093,\n",
              " 'eval_recall': 0.9998427734758853,\n",
              " 'eval_f1': 0.999823123636578,\n",
              " 'eval_runtime': 143.347,\n",
              " 'eval_samples_per_second': 226.792,\n",
              " 'eval_steps_per_second': 7.088,\n",
              " 'epoch': 4.998961010553946}"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer.push_to_hub()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

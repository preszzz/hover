{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U datasets evaluate optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import ASTFeatureExtractor, ASTForAudioClassification, ASTConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = 'username/my_test_audio_dataset'\n",
    "CACHE_DIR = './cache'\n",
    "\n",
    "# --- Audio Processing Parameters ---\n",
    "TARGET_SAMPLE_RATE = 16000  # Hz (16kHz)\n",
    "CHUNK_LENGTH_MS = 1000      # milliseconds (1 second)\n",
    "CHUNK_LENGTH_SAMPLES = int(TARGET_SAMPLE_RATE * CHUNK_LENGTH_MS / 1000)\n",
    "\n",
    "# --- Training Hyperparameters ---\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 5\n",
    "LR = 3e-5\n",
    "\n",
    "# --- Model Configuration ---\n",
    "MODEL_CHECKPOINT = \"MIT/ast-finetuned-audioset-10-10-0.4593\"\n",
    "CHECKPOINT_FILENAME = \"ast_best_model\"\n",
    "\n",
    "# --- Hyperparameter Config ---\n",
    "STUDY_NAME = \"drone-audio-detection-05-17\"\n",
    "N_TRIALS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_splits(dataset_name: str) -> Dataset:\n",
    "    \"\"\"Load a dataset from the Hugging Face Hub.\n",
    "\n",
    "    Args:\n",
    "        dataset_name: Name of the dataset to load\n",
    "\n",
    "    Returns:\n",
    "        Hugging Face Dataset object\n",
    "    \"\"\"\n",
    "    try:\n",
    "        dataset = load_dataset(dataset_name, cache_dir=CACHE_DIR)\n",
    "\n",
    "        print(f\"Dataset loaded successfully with splits: {list(dataset.keys())}\")\n",
    "        print(f\"Train split size: {dataset['train'].num_rows}\")\n",
    "        if 'valid' in dataset:\n",
    "            print(f\"Validation split size: {dataset['valid'].num_rows}\")\n",
    "        if 'test' in dataset:\n",
    "            print(f\"Test split size: {dataset['test'].num_rows}\")\n",
    "\n",
    "        return dataset\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load dataset {dataset_name}: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transformer_model(num_classes: int, model_checkpoint: str):\n",
    "    \"\"\"\n",
    "    Loads a pre-trained Audio Spectrogram Transformer (AST) model\n",
    "    from Hugging Face for PyTorch.\n",
    "\n",
    "    Args:\n",
    "        num_classes (int): The number of output classes for the classification layer.\n",
    "        model_checkpoint (str): The Hugging Face AST model identifier.\n",
    "\n",
    "    Returns:\n",
    "        torch.nn.Module: The PyTorch AST model with a potentially resized classification head.\n",
    "    \"\"\"\n",
    "    # Load the pre-trained AST model\n",
    "    # Set ignore_mismatched_sizes=True to allow replacing the classification head\n",
    "    label = {'not_drone': 0, 'drone': 1}\n",
    "\n",
    "    ast_config = ASTConfig.from_pretrained(model_checkpoint)\n",
    "\n",
    "    ast_config.num_labels = num_classes\n",
    "    ast_config.label2id = label\n",
    "    ast_config.id2label = {v: k for k, v in label.items()}\n",
    "\n",
    "    model = ASTForAudioClassification.from_pretrained(\n",
    "        model_checkpoint,\n",
    "        cache_dir=CACHE_DIR,\n",
    "        config=ast_config,\n",
    "        ignore_mismatched_sizes=True,\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_extractor(model_checkpoint: str):\n",
    "    \"\"\"\n",
    "    Loads the AST feature extractor.\n",
    "\n",
    "    Args:\n",
    "        model_checkpoint (str): The Hugging Face AST model identifier.\n",
    "\n",
    "    Returns:\n",
    "        ASTFeatureExtractor: The AST feature extractor instance.\n",
    "    \"\"\"\n",
    "    # Load the feature extractor\n",
    "    try:\n",
    "        feature_extractor = ASTFeatureExtractor.from_pretrained(model_checkpoint)\n",
    "        return feature_extractor\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading feature extractor: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    feature_extractor = get_feature_extractor(MODEL_CHECKPOINT)\n",
    "    print(f\"Successfully loaded feature extractor: {feature_extractor}\")\n",
    "    print(f\"Target sampling rate from feature extractor: {feature_extractor.sampling_rate} Hz\")\n",
    "    print(f\"Feature extractor expects max_length: {feature_extractor.max_length} samples\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load feature extractor: {e}\", exc_info=True)\n",
    "    feature_extractor = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_features(example):\n",
    "    \"\"\"Applies the AST feature extractor to a batch of audio data.\n",
    "\n",
    "    This function is designed to be used with `dataset.with_transform()`.\n",
    "\n",
    "    Args:\n",
    "        batch: A dictionary representing a batch of examples from the HF dataset.\n",
    "        Expected to contain an 'audio' key with audio data.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the processed 'input_values'.\n",
    "    \"\"\"\n",
    "    audio_arrays = [x[\"array\"] for x in example['input_values']]\n",
    "\n",
    "    # Apply the feature extractor\n",
    "    inputs = feature_extractor(\n",
    "        audio_arrays,\n",
    "        sampling_rate=feature_extractor.sampling_rate,\n",
    "        max_length=CHUNK_LENGTH_SAMPLES,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    if \"input_values\" in inputs:\n",
    "        example[\"input_values\"] = inputs[\"input_values\"]\n",
    "    else:\n",
    "        print(f\"Feature extractor output did not contain expected keys ('input_values'). Found: {inputs.keys()}\")\n",
    "        raise KeyError(\"Could not find processed features in feature extractor output.\")\n",
    "\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"CUDA available. Using GPU.\")\n",
    "        return torch.device(\"cuda\")\n",
    "    else:\n",
    "        print(\"CUDA/MPS not available. Using CPU.\")\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "DEVICE = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback, Trainer, TrainingArguments\n",
    "\n",
    "class CustomCallback(TrainerCallback):\n",
    "\n",
    "  def __init__(self, trainer) -> None:\n",
    "      super().__init__()\n",
    "      self._trainer = trainer\n",
    "\n",
    "  def on_train_begin(self, args, state, control, **kwargs):\n",
    "    self._trainer.init_hf_repo()\n",
    "\n",
    "  def on_train_end(self, args, state, control, **kwargs):\n",
    "    self._trainer.push_to_hub()\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate.load(\"accuracy\")\n",
    "recall = evaluate.load(\"recall\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "\n",
    "AVERAGE = \"binary\"\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits = eval_pred.predictions\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    metrics = accuracy.compute(predictions=predictions, references=eval_pred.label_ids)\n",
    "    metrics.update(precision.compute(predictions=predictions, references=eval_pred.label_ids, average=AVERAGE))\n",
    "    metrics.update(recall.compute(predictions=predictions, references=eval_pred.label_ids, average=AVERAGE))\n",
    "    metrics.update(f1.compute(predictions=predictions, references=eval_pred.label_ids, average=AVERAGE))\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init():\n",
    "    \"\"\"Initializes a new model for each Optuna trial.\"\"\"\n",
    "    model = build_transformer_model(num_classes=2, model_checkpoint=MODEL_CHECKPOINT)\n",
    "    return model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Optuna Hyperparameter Space Definition ---\n",
    "def optuna_hp_space(trial):\n",
    "    \"\"\"Defines the hyperparameter search space for Optuna.\"\"\"\n",
    "    return {\n",
    "        \"hub_model_id\": f\"preszzz/{STUDY_NAME}-trial-{trial.number}\",\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-5, 1e-3, log=True),\n",
    "        \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [8, 16, 32]),\n",
    "        \"weight_decay\": trial.suggest_float(\"weight_decay\", 1e-6, 1e-3, log=True),\n",
    "        \"warmup_ratio\": trial.suggest_float(\"warmup_ratio\", 0.0, 0.2),\n",
    "        \"lr_scheduler_type\": trial.suggest_categorical(\"lr_scheduler_type\", [\"linear\", \"cosine\", \"polynomial\"]),\n",
    "        \"max_grad_norm\": trial.suggest_float(\"max_grad_norm\", 0.1, 1.0),\n",
    "        \"optim\": trial.suggest_categorical(\"optim\", [\"adamw_torch\", \"adafactor\", \"adamw_torch_fused\"])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optuna_hp_name(trial):\n",
    "    return f\"{STUDY_NAME}_trial_{trial.number}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset_splits(DATASET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.rename_column('audio', 'input_values')\n",
    "processed_datasets = ds.with_transform(preprocess_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    eval_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    num_train_epochs=EPOCHS,\n",
    "    logging_steps=10,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    gradient_accumulation_steps=4,\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=True,\n",
    "    disable_tqdm=False\n",
    "    # push_to_hub=True,\n",
    "    # save_total_limit=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the trainer\n",
    "trainer = Trainer(\n",
    "    model_init=model_init,\n",
    "    args=args,\n",
    "    train_dataset=processed_datasets[\"train\"],\n",
    "    eval_dataset=processed_datasets[\"test\"],\n",
    "    processing_class=feature_extractor,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.add_callback(CustomCallback(trainer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trial_results = trainer.hyperparameter_search(\n",
    "    direction=\"maximize\",       # We want to maximize accuracy\n",
    "    backend=\"optuna\",\n",
    "    hp_space=optuna_hp_space,\n",
    "    hp_name=optuna_hp_name,\n",
    "    n_trials=N_TRIALS,\n",
    "    study_name=STUDY_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_trial_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

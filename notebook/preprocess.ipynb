{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Data Cleaning Pipeline\n",
    "\n",
    "This notebook implements the same preprocessing pipeline as the original codebase but adapts it for audio data from HuggingFace datasets. The pipeline includes:\n",
    "\n",
    "## Pipeline Overview\n",
    "\n",
    "1. Loading multiple subsets from datasets on HuggingFace\n",
    "2. Combining the dataset\n",
    "3. Splitting into train/validation/test sets\n",
    "4. Resampling audio to 16kHz\n",
    "5. Processing audio into uniform 1-second chunks\n",
    "6. Normalizing audio to consistent volume level\n",
    "8. Filtering invalid or silent clips\n",
    "9. Pushing processed dataset back to HuggingFace\n",
    "\n",
    "**Note:** This notebook is only a preprocessing assistant tool for model training and will not be used in the production environment. The final system will be deployed on a Raspberry Pi-like device for continuous environmental sound monitoring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Imports\n",
    "\n",
    "First, we import the necessary libraries for audio processing and dataset handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# !pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from datasets import load_dataset, DatasetDict, Audio\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the main repository\n",
    "repo_id = \"username/my_test_audio_dataset\"\n",
    "\n",
    "# Load configurations subset from main repository\n",
    "config_names = [\n",
    "\n",
    "]\n",
    "\n",
    "# Load and combine datasets\n",
    "all_subsets = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Parameters\n",
    "\n",
    "Define key parameters for audio processing, feature extraction, and parallelization. These parameters determine the chunk size, audio characteristics, and feature dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration settings\n",
    "# Audio Processing Parameters\n",
    "TARGET_SR = 16000\n",
    "CHUNK_DURATION_MS = 1000      # milliseconds\n",
    "CHUNK_LENGTH = int(TARGET_SR * CHUNK_DURATION_MS / 1000)\n",
    "\n",
    "NORMALIZATION_DB = -20.0\n",
    "\n",
    "# Multiprocess\n",
    "NUM_PROC = 48\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Process HuggingFace Dataset\n",
    "\n",
    "Now we'll implement the main processing pipeline for HuggingFace datasets. You can modify the dataset name and configuration as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process datasets with resampling to target sample rate\n",
    "for config in config_names:\n",
    "    try:\n",
    "        ds = load_dataset(repo_id, config, split=\"train\", cache_dir='./cache')\n",
    "        all_subsets[config] = ds\n",
    "        print(f\"Successfully loaded dataset {config} with {ds.num_rows} examples\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {config}: {e}\")\n",
    "\n",
    "all_subsets = DatasetDict(all_subsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_subsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine Processed Datasets\n",
    "\n",
    "After processing each dataset individually, we combine them into a single unified dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "datasets_to_concatenate = [all_subsets[key] for key in all_subsets.keys()]\n",
    "\n",
    "combined_dataset = concatenate_datasets(datasets_to_concatenate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(combined_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Splitting\n",
    "\n",
    "Split the combined dataset into train, validation, and test sets while maintaining the same class distribution in each split through stratification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_split = combined_dataset.train_test_split(test_size=0.10, stratify_by_column=\"label\")\n",
    "second_split = first_split[\"test\"].train_test_split(test_size=0.50, stratify_by_column=\"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Processed Dataset to HuggingFace\n",
    "\n",
    "Upload the final processed dataset back to HuggingFace for use in model training. The dataset is saved with a 2GB shard size to manage file sizes appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset = DatasetDict({\n",
    "    \"train\": first_split[\"train\"],\n",
    "    \"valid\": second_split[\"train\"],\n",
    "    \"test\": second_split[\"test\"],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio Processing Utilities\n",
    "\n",
    "Define utility functions for audio normalization and feature extraction. These functions handle:\n",
    "\n",
    "1. Audio normalization to a target dB level\n",
    "2. Detection and skipping of silent clips\n",
    "3. Prevention of clipping \n",
    "4. Extraction of MFCC features and their derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils\n",
    "def normalize_audio(signal: np.ndarray, label: int, target_db: float) -> tuple[np.ndarray, bool]:\n",
    "    \"\"\"\n",
    "    Normalize audio to target dB level and detect silence.\n",
    "    Returns (normalized_signal, success_flag)\n",
    "    \"\"\"\n",
    "    # Convert to int16 for silence check\n",
    "    signal_int16 = (signal * np.iinfo(np.int16).max).astype(np.int16)\n",
    "    if np.all(signal_int16 == 0):\n",
    "        if label == 1:\n",
    "            logging.warning(f\"Skipping empty signal for label {label}\")\n",
    "            return signal, False\n",
    "        else:\n",
    "            return signal, True\n",
    "\n",
    "    # Handle silent/empty signals\n",
    "    if np.max(np.abs(signal)) < 1e-10:\n",
    "        if label == 1:\n",
    "            logging.warning(f\"Skipping silent signal for label {label}\")\n",
    "            return signal, False\n",
    "        else:\n",
    "            logging.warning(f\"Skipping silent signal for label {label}\")\n",
    "            return signal, True\n",
    "\n",
    "    try:\n",
    "        # Calculate current RMS and dB\n",
    "        rms = np.sqrt(np.mean(signal ** 2))\n",
    "        current_db = 20 * np.log10(max(rms, 1e-10))\n",
    "\n",
    "        # Calculate gain needed\n",
    "        gain_db = target_db - current_db\n",
    "        gain_factor = 10 ** (gain_db / 20)\n",
    "\n",
    "        # Apply gain\n",
    "        normalized_signal = signal * gain_factor\n",
    "\n",
    "        # Prevent clipping if needed\n",
    "        if np.max(np.abs(normalized_signal)) > 0.95:  # Using 0.95 as a safety margin\n",
    "            normalized_signal = 0.95 * normalized_signal / np.max(np.abs(normalized_signal))\n",
    "\n",
    "        # Validate result\n",
    "        if not np.isfinite(normalized_signal).all():\n",
    "            logging.warning(\"Invalid normalized signal\")\n",
    "            return signal, False\n",
    "\n",
    "        return normalized_signal, True\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Normalization failed: {e}\")\n",
    "        return signal, False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunk Processing Function\n",
    "\n",
    "This function is the core of our preprocessing pipeline. It processes audio in batches and:\n",
    "\n",
    "1. Handles short clips by padding with zeros\n",
    "2. Splits longer clips into 1-second chunks\n",
    "3. Normalizes audio volume\n",
    "4. Extracts MFCC and delta features\n",
    "5. Filters out invalid or silent clips\n",
    "6. Preserves original labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_chunk(batch):\n",
    "    \"\"\"\n",
    "    Takes a resampled, mono audio array and:\n",
    "    - If shorter than clip_len: pads with zeros\n",
    "    - If longer than clip_len: extracts overlapping windows\n",
    "    - Normalizes audio to target dB level\n",
    "    - Filters out silent drone clips\n",
    "    Returns a list of fixed-size clips.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        all_audio = batch[\"audio\"]\n",
    "        all_labels = batch[\"label\"]\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Error loading batch: {e}\")\n",
    "\n",
    "    chunk_clips = []\n",
    "    chunk_labels = []\n",
    "\n",
    "    try:\n",
    "        for audio, label in zip(all_audio, all_labels):\n",
    "            try:\n",
    "                y = audio[\"array\"]\n",
    "                sr = audio[\"sampling_rate\"]\n",
    "                n = len(y)\n",
    "\n",
    "                # Validate audio format\n",
    "                if y.ndim != 1:\n",
    "                    logging.error(f\"Skipping sample: {audio['path']} with invalid number of channels {y.ndim}\")\n",
    "\n",
    "                if sr != TARGET_SR:\n",
    "                    logging.error(f\"Skipping sample: {audio['path']} with invalid sampling rate {sr}\")\n",
    "\n",
    "                # Handle audio file shorter than CHUNK_LENGTH\n",
    "                if n <= CHUNK_LENGTH:\n",
    "                    if n < CHUNK_LENGTH // 2:\n",
    "                        logging.warning(f\"Skipping sample: {audio['path']} with too short length {len(y)}\")\n",
    "                        continue\n",
    "\n",
    "                    # pad short signals with 0 at the random position\n",
    "                    pad_width = CHUNK_LENGTH - n\n",
    "                    pad_left = random.randint(0, pad_width)\n",
    "                    pad_right = pad_width - pad_left\n",
    "                    y_pad = np.pad(y, (pad_left, pad_right), mode=\"constant\")\n",
    "\n",
    "                    # Normalize and add if valid\n",
    "                    normalized_signal, norm_success = normalize_audio(y_pad, label, NORMALIZATION_DB)\n",
    "                    if norm_success:\n",
    "                        chunk_clips.append({\n",
    "                            \"array\": normalized_signal,\n",
    "                            \"sampling_rate\": sr\n",
    "                        })\n",
    "                        chunk_labels.append(label)\n",
    "\n",
    "\n",
    "                # Handle longer clips (chunk into segments)\n",
    "                else:\n",
    "                    num_full_chunks = n // CHUNK_LENGTH\n",
    "                    for i in range(num_full_chunks):\n",
    "                        start = i * CHUNK_LENGTH\n",
    "                        end = start + CHUNK_LENGTH\n",
    "                        chunk_data = y[start:end]\n",
    "\n",
    "                        # Normalize and add if valid\n",
    "                        normalized_signal, norm_success = normalize_audio(chunk_data, label, NORMALIZATION_DB)\n",
    "                        if norm_success:\n",
    "                            chunk_clips.append({\n",
    "                                \"array\": normalized_signal,\n",
    "                                \"sampling_rate\": sr\n",
    "                            })\n",
    "                            chunk_labels.append(label)\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"Error processing individual sample {audio['path']}: {e}\")\n",
    "                continue\n",
    "\n",
    "        return {\n",
    "            \"audio\": chunk_clips,\n",
    "            \"label\": chunk_labels,\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing batch: {e}\")\n",
    "        return {\n",
    "            \"audio\": [],\n",
    "            \"label\": []\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Preprocessing Loop\n",
    "\n",
    "Now we'll process each dataset in parallel using the HuggingFace datasets library. This applies our preprocessing to each audio file, resulting in a dataset with uniform chunk sizes and consistent features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast audio column to resample audio to targer sample rate\n",
    "final_dataset = final_dataset.cast_column(\"audio\", Audio(sampling_rate=TARGET_SR, mono=True))\n",
    "# Perform audio preprocess\n",
    "final_dataset = final_dataset.map(\n",
    "    process_chunk,\n",
    "    batched=True,\n",
    "    batch_size=500,\n",
    "    num_proc=NUM_PROC,\n",
    "    remove_columns=final_dataset[\"train\"].column_names\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Imbalance Analysis\n",
    "\n",
    "Check the class distribution to understand the balance between drone and non-drone sounds in our dataset. This helps determine if we need to apply balancing techniques during model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drone_label_counts = combined_dataset.filter(lambda example: example['label'] == 1).num_rows\n",
    "no_drone_label_counts = combined_dataset.filter(lambda example: example['label'] == 0).num_rows\n",
    "\n",
    "total = combined_dataset.num_rows\n",
    "minority_share = min(drone_label_counts, no_drone_label_counts) / total\n",
    "imb_ratio      = max(drone_label_counts, no_drone_label_counts) / min(drone_label_counts, no_drone_label_counts)\n",
    "\n",
    "print(f\"Minority share: {minority_share:.3%}\")\n",
    "print(f\"Imbalance ratio IR: {imb_ratio:.1f}:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Drone label count: {drone_label_counts}\")\n",
    "print(f\"No Drone label count: {no_drone_label_counts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset.push_to_hub('username/my_test_audio_dataset', commit_message=\"processed dataset\", max_shard_size=\"2GB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
